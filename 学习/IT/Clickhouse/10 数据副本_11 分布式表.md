# 10 数据副本/11 分布式表

## 集群机制

作用：

1. 数据副本
2. 分布式表

## 集群副本

数据冗余备份，高可用，数据副本

可多机组成集群，各节点平等关系，不分主从，集群的协调者使用zookeeper

集群副本只针对与MergeTree表引擎：Replicated*MergeTree，设置对象是表

同步过程：

1. 客户端往节点a写数据
2. 节点a响应客户端
3. 之后节点a通过异步的方式将数据同步到其他节点

注意事项：

1. 数据的同步是有延迟的
2. 对于insert语句，只会等待一个副本成功写入即返回；多个副本时，不保证所有副本的成功同步
3. 写入数据，单个数据块具有原子性，但不保证多个数据块写入的原子性
4. 数据块写入时会去重，避免多次执行相同语句
5. 基本不需要担心zookeeper的性能问题（例：一个Zookeeper处理Yandex.Metrica集群的300台服务器，可以支撑协调每秒几百个INSERT的场景）
6. 生产环境必配副本

实操：

1. 搭建zookeeper
2. clickhouse配置zookeeper连接（配置目录：config.xml）
3. 创建复制表（两个传参：zookeep路径（所有节点一致），节点名称（唯一）），可以使用宏（macros）（例：${replica}），配置在配置文件，相当于一个env这样的环境变量配置。
4. 建表需要同时在所有副本中执行

## 分布式表

表数据分布到多节点上，解决单机的问题，横向扩容（分片）。

使用Distribute表引擎，本身不会存储数据，只会分发到对应分片上执行。

> 由于clickhouse大量数据（30亿数据200G硬盘）以及大量查询（30亿数据秒级别）下表现优秀，而且分布式表需要网络资源，表现可能更差，所以大部分场景下都不需要。
>
> 由于网络的可靠性差，通常需要很重的设计进行弥补。但是Clickhouse极简的设计思想，可能存在一定问题。

### 数据写入机制

根据分片键分配到对应的分片上(shard)，可配置权重

internal_replication = false，需要distribute主动写到分片各自的节点上；设置成true，则distribute只往一个分片节点写入数据，而后续的同步工作交由复制表完成（Replicate*MergeTree）。推荐设置为true，内部有重试机制。

- 注：一个分片会有多个内部节点（集群副本）

### 数据读取机制

查询会在每个分片中执行

分片中的节点选择，默认根据出错次数低优先进行选择。可修改其他算法（user.xml中load_balancing配置）

### 实操

1. 配置集群，config.xml remote_servers下配置。一个集群可以有多个切片shard，一个切片可以有多个副本replica。clickhouse集群不建议经常变动。
2. 集群中建表，建表语句中使用on cluster关键字指定集群。（会同步到所有节点/副本上）

    使用Replicate*MergeTree时，指定zk路径（不同分片对应不同路径）和节点编号，需要使用配置的宏macro，因为指令执行会同步到所有节点，因此不能直接写死指定。 ← 宏的重要性

3. 建立分布式表 distributed(cluster_name, 数据库, 本地表（实际存储的表）, 分片键)（会同步到所有节点/副本上）
4. 使用时，使用distributed表。
